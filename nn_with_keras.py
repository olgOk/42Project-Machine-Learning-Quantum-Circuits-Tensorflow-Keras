# -*- coding: utf-8 -*-
"""NN_with_Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOuFSBYUAN1ekT4Qsf2o1sI0aCLIvisV

Author vokrut--42Robotics
"""

# Commented out IPython magic to ensure Python compatibility.
#Import required frameworks

import math

from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from datetime import datetime
# %tensorflow_version 1.x
# %load_ext tensorboard
import tensorflow as tf
from tensorflow.python.data import Dataset
from pandas.io.json import json_normalize
import json
from tensorflow import keras
from tensorflow.keras import layers
from google.colab import drive
drive.mount('/content/drive')

#upload json data file 
quantum_data = pd.read_json("drive/My Drive/qc/01/3125_random_5_qubit_circuits.json",
                            orient='records', lines=True, precise_float="precise_float")

# quantum_data.shape

def gate_data_preproc(quantum_data, gate_num):
  #since we care only about the value of the angles in U3 Gate, check for this type of gate
  #if the gate is not a U3 gate, set the angle values to 0
  is_not_u3gate = lambda data: data["Gate_Type"] != 'U3Gate'
  norm_data = json_normalize(quantum_data)
  dataframe = pd.DataFrame()  #create new data table

  #.loc[] - accesses a group of rows and columns by label(s) or a boolean array.
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_1')] = 0.0
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_2')] = 0.0
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_3')] = 0.0

  #iterate through each subcateggory, pass if GateType or GateNumber
  #store in the new table 
  for key in norm_data:
    # print(key)
    if key == 'Gate_Number' or key == 'Gate_Type':
      continue
    dataframe[key+gate_num] = norm_data[key]
    if key in ['Target', 'Control']:
      dataframe[key+gate_num] += 1
      # print(dataframe[key+gate_num])

  return dataframe

norm_qdata = json_normalize(quantum_data['statevector'])
# print(qdata.head())
#iterate in data set. where key represent a gate number on each iteration
for key in quantum_data:
  # print(key)
  if "gate" in key:                   #if key == gate, store the gate number
    gate_num = "_" + str(key[-2:])
    gate_dataframe = gate_data_preproc(quantum_data[key], gate_num)
    norm_qdata = norm_qdata.join(gate_dataframe)

#randomize data
# norm_qdata = norm_qdata.reindex(
#     np.random.permutation(norm_qdata.index))
#display first 10 from normalized data
norm_qdata
# norm_qdata.shape

#Define and store the input features

def preprocess_features(quantum_data):
  #define features
  selected_features = quantum_data.filter(regex='^(?!state)', axis=1) #!define later
  return selected_features

#define and store the target
def preprocess_targets(quantum_data):
  output_targets = pd.DataFrame()
  output_targets = quantum_data.filter(regex='^state', axis=1) 
  output_targets /= 1024
  return output_targets

#display to male sure you are thinking correct
# display.display(preprocess_features(norm_qdata))
# display.display(preprocess_targets(norm_qdata))

feature = preprocess_features(norm_qdata)
target = preprocess_targets(norm_qdata)

# display.display(preprocess_features(norm_qdata))
# display.display(preprocess_targets(norm_qdata))

feature.shape
target.shape

#create input layer
inputs = keras.Input(shape=(205,))

#first layer with 1024 units, activation function is relu
layer1 = layers.Dense(1024, activation='relu')(inputs)

#second layer with 512 units, activation function is relu
layer2 = layers.Dense(512, activation='relu')(layer1)

#third layer with 256 units, activation function is relu
layer3 = layers.Dense(525612, activation='relu')(layer2)
#output layer activation function is softmax
outputs = layers.Dense(32, activation='softmax')(layer3)

model = keras.Model(inputs=inputs, outputs=outputs, name='predict_statevector')

# Let's check out what the model summary looks like:

model.summary()
keras.utils.plot_model(model, 'my_first_model.png')

#set training data. For now, takes only first 100 items from the data set. Increase the size later
training_feature = preprocess_features(norm_qdata.head(2000))
training_target = preprocess_targets(norm_qdata.head(2000))

#set validating data.For now, takes only last 100 items from the data set. Increase the size later
validating_feature = preprocess_features(norm_qdata.tail(100))
validating_target = preprocess_targets(norm_qdata.tail(100))

print(training_feature.shape)
print(training_target.shape)

def my_log_error_fn(predict, actual):
  # raws = predict.K.shape[0]
  raws = predict.shape[0]
  col = predict.shape[1]
  it = np.nditer([predict, actual])
  err = 0.0
  with it:
    for (x, y) in it:
      err += (y*np.log(x) + (1 - y)*np.log(1 - x))

  return -err / ( col * raws)

#after data preprocessing and defining nn, compile the model
model.compile(loss='mse',
              optimizer='adam',
              metrics=['accuracy'])

#train the model on specidied data. Save log for later to see the result in Tensorboard

logdir = "logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)

training_history = model.fit(training_feature,
                             training_target, 
                             batch_size=32,
                             epoch=1,
                             verbose=2,
                             validation_data=(validating_feature, validating_target),
                             callbacks=[tensorboard_callback])

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/scalars

model.evaluate(x=validating_feature, y=validating_target, batch_size=32, verbose=2)
model.metrics_names
# history = model.fit_generator(
#         train_generator,
#         steps_per_epoch=train_generator.n/batch_size,
#         epochs=10)

# plt.plot(history.history['loss'])
# plt.title('loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['loss'], loc='upper left')
# plt.show()