# -*- coding: utf-8 -*-
"""NN_with_Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NOuFSBYUAN1ekT4Qsf2o1sI0aCLIvisV

Author vokrut--42Robotics
"""

# Commented out IPython magic to ensure Python compatibility.
#Import required frameworks
import math
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
from datetime import datetime
# %tensorflow_version 1.x
# %load_ext tensorboard
import tensorflow as tf
from tensorflow.python.data import Dataset
from pandas.io.json import json_normalize
import json
import keras
from keras import layers

from google.colab import drive
drive.mount('/content/drive')

#upload json data file 
quantum_data = pd.read_json("drive/My Drive/qc/01/3125_random_5_qubit_circuits.json",
                            orient='records', lines=True, precise_float="precise_float")

# quantum_data.shape

def gate_data_preproc(quantum_data: dict,
                      gate_num: int) -> pd.DataFrame:
  """
  Function takes the raw data as a .json file, split the data into two parts, 
  the first 32 columns represent state vectors the last 173 colums represent
  the parameters of each gate.

  Parameters: quantum_data: dict - the raw data read from .json file
              gate_num: int - current gate number from the range from  1 to 40
  Returns:    a new pandas dataframe with separated values of the statevectors and
              gate's parameters

  """
  #since we care only about the value of the angles in U3 Gate, check for this type of gate
  #if the gate is not a U3 gate, set the angle values to 0
  is_not_u3gate = lambda data: data["Gate_Type"] != 'U3Gate'
  norm_data = json_normalize(quantum_data)
  dataframe = pd.DataFrame()  #create new data table

  #.loc[] - accesses a group of rows and columns by label(s) or a boolean array.
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_1')] = 0.0
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_2')] = 0.0
  norm_data.loc[(is_not_u3gate(norm_data), 'Angle_3')] = 0.0

  #iterate through each subcateggory, pass if GateType or GateNumber
  #store in the new table 
  for key in norm_data:
    # print(key)
    if key == 'Gate_Number' or key == 'Gate_Type':
      continue
    dataframe[key+gate_num] = norm_data[key]
    if key in ['Target', 'Control']:
      dataframe[key+gate_num] += 1
      # print(dataframe[key+gate_num])

  return dataframe

norm_qdata = json_normalize(quantum_data['statevector'])
#iterate in data set. where key represent a gate number on each iteration
for key in quantum_data:
  # print(key)
  if "gate" in key:                   #if key == gate, store the gate number
    gate_num = "_" + str(key[-2:])
    gate_dataframe = gate_data_preproc(quantum_data[key], gate_num)
    norm_qdata = norm_qdata.join(gate_dataframe)

#randomize data
# norm_qdata = norm_qdata.reindex(
#     np.random.permutation(norm_qdata.index))
#display first 10 from normalized data
norm_qdata
# norm_qdata.shape

def preprocess_features(quantum_data: pd.DataFrame) -> pd.DataFrame:
  """
  Define and store the input features.
  Parameters:   normalized dataframe
  Returns:      features, e.g. last 173 colums of the dataframe as a pandas
                dataframe type
  """
  selected_features = quantum_data.filter(regex='^(?!state)', axis=1) 
  return selected_features

def preprocess_targets(quantum_data: pd.DataFrame) -> pd.DataFrame:
  """
  Define and store the targets
  Parameters:   normalized dataframe
  Returns:      targets, e.g. first 32 colums of the dataframe as a Pandas
                Dataframe object
  """

  output_targets = pd.DataFrame()
  output_targets = quantum_data.filter(regex='^state', axis=1) 
  output_targets /= 1024
  return output_targets

#display to male sure you are thinking correct
# display.display(preprocess_features(norm_qdata))
# display.display(preprocess_targets(norm_qdata))

feature = preprocess_features(norm_qdata)
target = preprocess_targets(norm_qdata)

# display.display(preprocess_features(norm_qdata))
# display.display(preprocess_targets(norm_qdata))

feature.shape
target.shape

#create input layer
inputs = keras.Input(shape=(205,))

#first layer with 1024 units, activation function is relu
layer1 = layers.Dense(1024, activation='relu')(inputs)

#second layer with 512 units, activation function is relu
layer2 = layers.Dense(512, activation='relu')(layer1)

#third layer with 256 units, activation function is relu
layer3 = layers.Dense(256, activation='relu')(layer2)

#output layer activation function is softmax
outputs = layers.Dense(32, activation='softmax')(layer3)

#define the keras model
model = keras.Model(inputs=inputs, outputs=outputs, name='predict_statevector')

# Let's check out what the model summary looks like:

model.summary()
keras.utils.plot_model(model, 'my_first_model.png')

#set training data. For now, takes only first 2000 items from the data set. Increase the size later
training_feature = preprocess_features(norm_qdata.head(2000))
training_target = preprocess_targets(norm_qdata.head(2000))

#set validating data.For now, takes only last 100 items from the data set. Increase the size later
validating_feature = preprocess_features(norm_qdata.tail(100))
validating_target = preprocess_targets(norm_qdata.tail(100))

print(training_feature.shape)
print(training_target.shape)

#after data preprocessing and defining nn, compile the model
#as a loss function uses categorical-crossentropy
#adam optimizer and accuract as metrics
#categorical_crossentropy

model.compile(loss='mean_squared_logarithmic_error',
              optimizer='adam',
              metrics=['accuracy'])

#@title

import os
from tensorflow.keras.callbacks import TensorBoard

class TrainValTensorBoard(TensorBoard):
    """
    keras tensorboard: plot train and validation scalars in a same figure
    https://stackoverflow.com/questions/47877475/keras-tensorboard-plot-train-and-validation-scalars-in-a-same-figure
    To handle the validation logs with a separate writer, you can write a custom callback that wraps around the original TensorBoard methods.
    """

    def __init__(self, log_dir='./logs', **kwargs):
        # Make the original `TensorBoard` log to a subdirectory 'training'
        training_log_dir = os.path.join(log_dir, 'training')
        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)

        # Log the validation metrics to a separate subdirectory
        self.val_log_dir = os.path.join(log_dir, 'validation')

    def set_model(self, model):
        # Setup writer for validation metrics
        self.val_writer = tf.summary.FileWriter(self.val_log_dir)
        super(TrainValTensorBoard, self).set_model(model)

    def on_epoch_end(self, epoch, logs=None):
        # Pop the validation logs and handle them separately with
        # `self.val_writer`. Also rename the keys so that they can
        # be plotted on the same figure with the training metrics
        logs = logs or {}
        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}
        for name, value in val_logs.items():
            summary = tf.Summary()
            summary_value = summary.value.add()
            summary_value.simple_value = value.item()
            summary_value.tag = name
            self.val_writer.add_summary(summary, epoch)
        self.val_writer.flush()

        # Pass the remaining logs to `TensorBoard.on_epoch_end`
        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}
        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)

    def on_train_end(self, logs=None):
        super(TrainValTensorBoard, self).on_train_end(logs)
        self.val_writer.close()

# Clear any logs from previous runs
!rm -rf ./logs

logdir = "logs"
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=50, write_graph=True, write_images=True)

#train the model on specidied data. Save logs for later to see the result
#in Tensorboard using callbacks

training_history = model.fit(x=training_feature,
                             y=training_target, 
                             batch_size=16,
                             nb_epoch=25,
                             verbose=2,
                             validation_data=(validating_feature, validating_target),
                             callbacks=[TrainValTensorBoard(write_graph=False)])
                            #  callbacks=[TensorBoardColabCallback(tbc)])

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/

plt.plot(training_history.history['loss'])
plt.plot(training_history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

#save the model
# The SavedModel files that were created contain:
#         A TensorFlow checkpoint containing the model weights.
#         A SavedModel proto containing the underlying TensorFlow graph.

# path = 'logs/my_keras_model.h5'
model.save('model_dir/my_keras_model.h5')

#retrieves the weights values as a list of Numpy arrays vie get_weights()
#set the state of the model via set_weights

saved_weights = model.get_weights()
model.set_weights(saved_weights)

# Recreate the exact same model
path = 'model_dir/my_new_model.h5'
new_model = keras.models.load_model(path)
predictions = model.predict(validating_feature)
# Check that the state is preserved
# This file includes:
#   The model's architecture
#   The model's weight values (which were learned during training)
#   The model's training config (what you passed to compile), if any
#   The optimizer and its state, if any (this enables you to restart training where you left off)
new_predictions = new_model.predict(validating_feature)
np.testing.assert_allclose(predictions, new_predictions, rtol=1e-6, atol=1e-6)

# Clear any logs from previous runs
!rm -rf ./logs